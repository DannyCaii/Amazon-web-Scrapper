{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-21T22:55:10.456006Z",
     "start_time": "2024-05-21T22:55:09.761655Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import datetime\n",
    "#Libary to send Emails to self\n",
    "import smtplib\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading content for https://www.amazon.com/Organic-Honeycrisp-Apple-One-Medium/dp/B001GIP2A8/ref=sr_1_3_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-3...\n",
      "https://www.amazon.com/Organic-Honeycrisp-Apple-One-Medium/dp/B001GIP2A8/ref=sr_1_3_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-3 content written to downloaded_html\\Organic-Honeycrisp-Apple-One-Medium.html\n",
      "Downloading content for https://www.amazon.com/Dole-Organic-Bananas-Bag/dp/B07ZLF9G83?pd_rd_i=B07ZLF9G83&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_1_i...\n",
      "https://www.amazon.com/Dole-Organic-Bananas-Bag/dp/B07ZLF9G83?pd_rd_i=B07ZLF9G83&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_1_i content written to downloaded_html\\Dole-Organic-Bananas-Bag.html\n",
      "Downloading content for https://www.amazon.com/produce-aisle-Strawberries-1-lb/dp/B000P6J0SM?pd_rd_i=B000P6J0SM&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_3_i...\n",
      "https://www.amazon.com/produce-aisle-Strawberries-1-lb/dp/B000P6J0SM?pd_rd_i=B000P6J0SM&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_3_i content written to downloaded_html\\produce-aisle-Strawberries-1-lb.html\n",
      "Downloading content for https://www.amazon.com/Shrimp-White-Farm-Raised-Frozen/dp/B07FZFB494?pd_rd_i=B07FZFB494&fpw=alm&almBrandId=VUZHIFdob2xlIEZvb2Rz&ref_=pd_alm_wf_dsk_dp_dzrp_1_6_i...\n",
      "https://www.amazon.com/Shrimp-White-Farm-Raised-Frozen/dp/B07FZFB494?pd_rd_i=B07FZFB494&fpw=alm&almBrandId=VUZHIFdob2xlIEZvb2Rz&ref_=pd_alm_wf_dsk_dp_dzrp_1_6_i content written to downloaded_html\\Shrimp-White-Farm-Raised-Frozen.html\n",
      "Downloading content for https://www.amazon.com/Seafood-Halibut-Fillet-Msc/dp/B07HMC7VGJ?pd_rd_i=B07HMC7VGJ&fpw=alm&almBrandId=VUZHIFdob2xlIEZvb2Rz&ref_=pd_alm_wf_dsk_dp_dzrp_1_2_i...\n",
      "https://www.amazon.com/Seafood-Halibut-Fillet-Msc/dp/B07HMC7VGJ?pd_rd_i=B07HMC7VGJ&fpw=alm&almBrandId=VUZHIFdob2xlIEZvb2Rz&ref_=pd_alm_wf_dsk_dp_dzrp_1_2_i content written to downloaded_html\\Seafood-Halibut-Fillet-Msc.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import codecs\n",
    "from urllib.parse import urlparse  # For filename extraction\n",
    "\n",
    "site_to_check = [\n",
    "                    #Fresh Produce\n",
    "                    'https://www.amazon.com/Organic-Honeycrisp-Apple-One-Medium/dp/B001GIP2A8/ref=sr_1_3_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-3',\n",
    "                    'https://www.amazon.com/Dole-Organic-Bananas-Bag/dp/B07ZLF9G83?pd_rd_i=B07ZLF9G83&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_1_i',\n",
    "                    'https://www.amazon.com/produce-aisle-Strawberries-1-lb/dp/B000P6J0SM?pd_rd_i=B000P6J0SM&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_3_i',\n",
    "                    'https://www.amazon.com/Shrimp-White-Farm-Raised-Frozen/dp/B07FZFB494?pd_rd_i=B07FZFB494&fpw=alm&almBrandId=VUZHIFdob2xlIEZvb2Rz&ref_=pd_alm_wf_dsk_dp_dzrp_1_6_i',\n",
    "                    'https://www.amazon.com/Seafood-Halibut-Fillet-Msc/dp/B07HMC7VGJ?pd_rd_i=B07HMC7VGJ&fpw=alm&almBrandId=VUZHIFdob2xlIEZvb2Rz&ref_=pd_alm_wf_dsk_dp_dzrp_1_2_i'\n",
    "                     ]\n",
    "\n",
    "\n",
    "def write_html_file(site, proxy=None, folder_path='downloaded_html', prettify=False):\n",
    "    \"\"\"\n",
    "    Attempts to download the content of a website using a proxy and writes it to a file\n",
    "    within a specified folder. Optionally, prettifies the downloaded HTML content.\n",
    "\n",
    "    Args:\n",
    "        site (str): The URL of the website to access.\n",
    "        proxy (str, optional): The proxy address (e.g., 'http://127.0.0.1:8080'). Defaults to None.\n",
    "        folder_path (str, optional): The path to the folder where HTML files will be saved. Defaults to 'downloaded_html'.\n",
    "        prettify (bool, optional): Whether to prettify the downloaded HTML content before writing (improves readability). Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Create the folder if it doesn't exist\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        # Extract filename between 2nd and 3rd backslashes (consider potential naming issues with special characters)\n",
    "        url_parts = urlparse(site).path.split('/')\n",
    "        if len(url_parts) >= 2:  # Ensure there are at least 2 parts (1 backslash)\n",
    "            filename = f\"{url_parts[1]}.html\"\n",
    "        else:\n",
    "            filename = f\"unknown_{site.split('/')[-1]}.html\"  # Fallback for malformed URLs\n",
    "\n",
    "        print(f'Downloading content for {site}...')\n",
    "\n",
    "        headers = {'User-Agent':\n",
    "                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',\n",
    "                   'Cache-Control': 'no-cache'}  # Disable caching in headers\n",
    "\n",
    "        response = requests.get(site, proxies={'http': proxy, 'https': proxy}, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Construct the full path to the file within the folder\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Downloaded content\n",
    "            html_content = response.text\n",
    "\n",
    "            if prettify:\n",
    "                # Prettify the HTML content using BeautifulSoup (optional)\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                pretty_html = soup.prettify()\n",
    "                html_content = pretty_html\n",
    "            else:\n",
    "                # Keep the content as-is\n",
    "                pass\n",
    "\n",
    "            with codecs.open(full_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(html_content)\n",
    "            print(f'{site} content written to {full_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {site} (Status code: {response.status_code})')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error downloading {site}: {e}')\n",
    "    \n",
    "\n",
    "# Site List of What you want to scrape\n",
    "if __name__ == '__main__':\n",
    "    sites_to_check = site_to_check\n",
    "\n",
    "    # Call the function for each website (optionally with a proxy)\n",
    "    for site in site_to_check:\n",
    "        write_html_file(site)  # Without proxy\n",
    "        # write_html_file(site, proxy='http://your_proxy_address:port')  # With proxy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T23:10:48.259933Z",
     "start_time": "2024-05-21T23:10:34.524294Z"
    }
   },
   "id": "909236c925dc692c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing price data from Dole-Organic-Bananas-Bag.html\n",
      "Data appended to existing CSV file.\n",
      "Processing price data from Organic-Honeycrisp-Apple-One-Medium.html\n",
      "Data appended to existing CSV file.\n",
      "Processing price data from produce-aisle-Strawberries-1-lb.html\n",
      "Data appended to existing CSV file.\n",
      "Processing price data from Seafood-Halibut-Fillet-Msc.html\n",
      "Data appended to existing CSV file.\n",
      "Processing price data from Shrimp-White-Farm-Raised-Frozen.html\n",
      "Data appended to existing CSV file.\n"
     ]
    }
   ],
   "source": [
    "#The Final Product\n",
    "\n",
    "import subprocess\n",
    "from datetime import date  # Import date for today's date\n",
    "from bs4 import BeautifulSoup  # Import BeautifulSoup for parsing HTML\n",
    "import csv  # Import csv library for writing\n",
    "import os\n",
    "\n",
    "from proxy_rotation import write_html_file\n",
    "\n",
    "\n",
    "def check_price():\n",
    "    # Run the proxy rotation (assuming proxy_rotation.py is a separate script)\n",
    "    subprocess.run([\"python\", \"proxy_rotation.py\"])\n",
    "\n",
    "    # Path to the folder containing HTML files\n",
    "    folder_path = \"downloaded_html\"\n",
    "\n",
    "    counter = 0 #Link Counter\n",
    "    # Iterate over files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        \n",
    "        \n",
    "        if filename.endswith(\".html\"):  # Check if it's an HTML file\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            try:\n",
    "                # Open the file and process its contents\n",
    "                with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                    html_content = f.read()\n",
    "\n",
    "                    # Process the HTML content (extract price, etc.)\n",
    "                    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "                    # Access and process the parsed HTML using BeautifulSoup methods\n",
    "                    title = soup.find(id='productTitle').text.strip()\n",
    "                    price = soup.find(class_='a-price-whole').text.strip() + soup.find(class_='a-price-fraction').text.strip()\n",
    "                    today = date.today()\n",
    "                    \n",
    "                    link = site_to_check[counter] \n",
    "                    \n",
    "                                       \n",
    "                \n",
    "                    print(f\"Processing price data from {filename}\")\n",
    "                    \n",
    "\n",
    "                    # ----------------------------------Writing into a csv ------------------------------------\n",
    "                    header = ['Title', 'Price', 'Date', 'Link']\n",
    "                    data = [[title, price, today, link]]\n",
    "\n",
    "                    # Check if the CSV file exists\n",
    "                    file_exists = os.path.isfile('amazon_web_scrapper_dataset.csv')\n",
    "\n",
    "                    if not file_exists:  # Create the CSV file if it doesn't exist\n",
    "                        with open('amazon_web_scrapper_dataset.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            writer.writerow(header)\n",
    "                            writer.writerows(data)\n",
    "                        print('CSV file created successfully!')\n",
    "                    else:  # Append data if the CSV file already exists\n",
    "                        with open('amazon_web_scrapper_dataset.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            writer.writerows(data)\n",
    "                        print('Data appended to existing CSV file.')\n",
    "\n",
    "            except Exception as e:  # Catch any exceptions during processing or writing\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "        counter += 1 #Link Counter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_price()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T23:16:27.616578Z",
     "start_time": "2024-05-21T23:16:27.145127Z"
    }
   },
   "id": "e821f9af12ad8547",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#-------------------------Automating the script into time intervals------------------------\n",
    "while(True):\n",
    "    check_price()\n",
    "    time.sleep(10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed228a0bec23b66c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"amazon_web_scrapper_dataset.csv\" cleared successfully!\n",
      "CSV file \"amazon_web_scrapper_dataset.csv\" header preserved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def clear_csv(filename, keep_header=False):\n",
    "  \"\"\"\n",
    "  Clears the contents of a CSV file, optionally keeping the header line.\n",
    "\n",
    "  Args:\n",
    "      filename (str): The path to the CSV file.\n",
    "      keep_header (bool, optional): Whether to keep the header line (default: False).\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    if os.path.exists(filename):\n",
    "      with open(filename, 'r', newline='') as f:\n",
    "        lines = f.readlines()  # Read all lines into a list\n",
    "\n",
    "      if keep_header:  # Check only if keeping the header\n",
    "        lines = lines[2:]  # Skip the first line (header) if requested\n",
    "\n",
    "      else:\n",
    "        lines = []  # Empty list to clear the content\n",
    "\n",
    "      with open(filename, 'w', newline='') as f:\n",
    "        f.writelines(lines)  # Write the modified lines back to the file\n",
    "\n",
    "      if keep_header:\n",
    "        print(f'CSV file \"{filename}\" header preserved!')\n",
    "      else:\n",
    "        print(f'CSV file \"{filename}\" cleared successfully!')\n",
    "    else:\n",
    "      print(f'CSV file \"{filename}\" does not exist.')\n",
    "  except Exception as e:\n",
    "    print(f'Error clearing CSV file: {e}')\n",
    "\n",
    "# Example usage (clear everything)\n",
    "clear_csv('amazon_web_scrapper_dataset.csv')\n",
    "\n",
    "# Example usage (keep the header)\n",
    "clear_csv('amazon_web_scrapper_dataset.csv', keep_header=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T02:18:33.742430Z",
     "start_time": "2024-05-21T02:18:33.735163Z"
    }
   },
   "id": "57b466008b2d131d",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "17e1fb9a10763d76"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
