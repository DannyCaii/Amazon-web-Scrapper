{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T22:45:14.268251Z",
     "start_time": "2024-05-22T22:45:13.253709Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import datetime\n",
    "#Libary to send Emails to self\n",
    "import smtplib\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading content for https://www.amazon.com/Organic-Honeycrisp-Apple-One-Medium/dp/B001GIP2A8/ref=sr_1_3_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-3...\n",
      "https://www.amazon.com/Organic-Honeycrisp-Apple-One-Medium/dp/B001GIP2A8/ref=sr_1_3_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-3 content written to downloaded_html\\Organic-Honeycrisp-Apple-One-Medium.html\n",
      "Downloading content for https://www.amazon.com/Dole-Organic-Bananas-Bag/dp/B07ZLF9G83?pd_rd_i=B07ZLF9G83&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_1_i...\n",
      "https://www.amazon.com/Dole-Organic-Bananas-Bag/dp/B07ZLF9G83?pd_rd_i=B07ZLF9G83&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_1_i content written to downloaded_html\\Dole-Organic-Bananas-Bag.html\n",
      "Downloading content for https://www.amazon.com/produce-aisle-Strawberries-1-lb/dp/B000P6J0SM?pd_rd_i=B000P6J0SM&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_3_i...\n",
      "https://www.amazon.com/produce-aisle-Strawberries-1-lb/dp/B000P6J0SM?pd_rd_i=B000P6J0SM&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_3_i content written to downloaded_html\\produce-aisle-Strawberries-1-lb.html\n",
      "Downloading content for https://www.amazon.com/Hass-Avocado-Large-Ready-Eat/dp/B000NOGKN4?pd_rd_i=B000NOGKN4&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_5_i...\n",
      "https://www.amazon.com/Hass-Avocado-Large-Ready-Eat/dp/B000NOGKN4?pd_rd_i=B000NOGKN4&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_5_i content written to downloaded_html\\Hass-Avocado-Large-Ready-Eat.html\n",
      "Downloading content for https://www.amazon.com/Fresh-Brand-Stoplight-Bell-Peppers/dp/B086WXFWP4/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2...\n",
      "https://www.amazon.com/Fresh-Brand-Stoplight-Bell-Peppers/dp/B086WXFWP4/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2 content written to downloaded_html\\Fresh-Brand-Stoplight-Bell-Peppers.html\n",
      "Downloading content for https://www.amazon.com/Fresh-Brand-Whole-Carrots-16/dp/B07XLV61C9/ref=sr_1_3_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-3...\n",
      "https://www.amazon.com/Fresh-Brand-Whole-Carrots-16/dp/B07XLV61C9/ref=sr_1_3_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-3 content written to downloaded_html\\Fresh-Brand-Whole-Carrots-16.html\n",
      "Downloading content for https://www.amazon.com/Church-Brothers-Farms-Broccoli-Florets/dp/B09GWDQZW3/ref=sr_1_5_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-5...\n",
      "https://www.amazon.com/Church-Brothers-Farms-Broccoli-Florets/dp/B09GWDQZW3/ref=sr_1_5_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-5 content written to downloaded_html\\Church-Brothers-Farms-Broccoli-Florets.html\n",
      "Downloading content for https://www.amazon.com/Christopher-Ranch-White-Garlic-pack/dp/B09B323RJV/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1...\n",
      "https://www.amazon.com/Christopher-Ranch-White-Garlic-pack/dp/B09B323RJV/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1 content written to downloaded_html\\Christopher-Ranch-White-Garlic-pack.html\n",
      "Downloading content for https://www.amazon.com/produce-aisle-Lemon-One-Medium/dp/B001L1KRNC/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1...\n",
      "https://www.amazon.com/produce-aisle-Lemon-One-Medium/dp/B001L1KRNC/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1 content written to downloaded_html\\produce-aisle-Lemon-One-Medium.html\n",
      "Downloading content for https://www.amazon.com/produce-aisle-mburring-Yellow-Onion/dp/B001W3T2SK/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1...\n",
      "https://www.amazon.com/produce-aisle-mburring-Yellow-Onion/dp/B001W3T2SK/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1 content written to downloaded_html\\produce-aisle-mburring-Yellow-Onion.html\n",
      "Downloading content for https://www.amazon.com/Tanimura-Antle-Italian-Parsley-Bunch/dp/B08731FV2H/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1...\n",
      "https://www.amazon.com/Tanimura-Antle-Italian-Parsley-Bunch/dp/B08731FV2H/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1 content written to downloaded_html\\Tanimura-Antle-Italian-Parsley-Bunch.html\n",
      "Downloading content for https://www.amazon.com/Tanimura-Antle-Cilantro-1-Bunch/dp/B08731HWZV/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1...\n",
      "https://www.amazon.com/Tanimura-Antle-Cilantro-1-Bunch/dp/B08731HWZV/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1 content written to downloaded_html\\Tanimura-Antle-Cilantro-1-Bunch.html\n",
      "Downloading content for https://www.amazon.com/Fresh-Brand-Organic-Basil-0-5/dp/B097F282FC/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2...\n",
      "https://www.amazon.com/Fresh-Brand-Organic-Basil-0-5/dp/B097F282FC/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2 content written to downloaded_html\\Fresh-Brand-Organic-Basil-0-5.html\n",
      "Downloading content for https://www.amazon.com/Fresh-Brand-Russet-Potatoes/dp/B07XW1TNXZ/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1...\n",
      "https://www.amazon.com/Fresh-Brand-Russet-Potatoes/dp/B07XW1TNXZ/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1 content written to downloaded_html\\Fresh-Brand-Russet-Potatoes.html\n",
      "Downloading content for https://www.amazon.com/Taylor-Farms-65107-Spinach-Bag/dp/B00KMM8I6Y/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2...\n",
      "https://www.amazon.com/Taylor-Farms-65107-Spinach-Bag/dp/B00KMM8I6Y/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2 content written to downloaded_html\\Taylor-Farms-65107-Spinach-Bag.html\n",
      "Downloading content for https://www.amazon.com/Fresh-Brand-Vine-Tomatoes/dp/B086WX15TH/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2...\n",
      "https://www.amazon.com/Fresh-Brand-Vine-Tomatoes/dp/B086WX15TH/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2 content written to downloaded_html\\Fresh-Brand-Vine-Tomatoes.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import codecs\n",
    "from urllib.parse import urlparse  # For filename extraction\n",
    "\n",
    "site_to_check = [\n",
    "                    #Fresh Produce Apples Bananas Strawberries Avocados Bell Peppers Carrots Broccoli Garlic Lemons/Limes Onion Parsley Cilantro Basil Potatoes Spinach Tomatoes\n",
    "                    \n",
    "                    'https://www.amazon.com/Organic-Honeycrisp-Apple-One-Medium/dp/B001GIP2A8/ref=sr_1_3_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-3',\n",
    "                    'https://www.amazon.com/Dole-Organic-Bananas-Bag/dp/B07ZLF9G83?pd_rd_i=B07ZLF9G83&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_1_i',\n",
    "                    'https://www.amazon.com/produce-aisle-Strawberries-1-lb/dp/B000P6J0SM?pd_rd_i=B000P6J0SM&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_3_i',\n",
    "                    'https://www.amazon.com/Hass-Avocado-Large-Ready-Eat/dp/B000NOGKN4?pd_rd_i=B000NOGKN4&fpw=alm&almBrandId=QW1hem9uIEZyZXNo&ref_=pd_alm_fs_dsk_sf_ai_16318981_1_5_i',\n",
    "                    'https://www.amazon.com/Fresh-Brand-Stoplight-Bell-Peppers/dp/B086WXFWP4/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2',\n",
    "                    'https://www.amazon.com/Fresh-Brand-Whole-Carrots-16/dp/B07XLV61C9/ref=sr_1_3_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-3',\n",
    "                    'https://www.amazon.com/Church-Brothers-Farms-Broccoli-Florets/dp/B09GWDQZW3/ref=sr_1_5_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-5',\n",
    "                    'https://www.amazon.com/Christopher-Ranch-White-Garlic-pack/dp/B09B323RJV/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1',\n",
    "                    'https://www.amazon.com/produce-aisle-Lemon-One-Medium/dp/B001L1KRNC/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1',\n",
    "                    'https://www.amazon.com/produce-aisle-mburring-Yellow-Onion/dp/B001W3T2SK/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1',\n",
    "                    'https://www.amazon.com/Tanimura-Antle-Italian-Parsley-Bunch/dp/B08731FV2H/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1',\n",
    "                    'https://www.amazon.com/Tanimura-Antle-Cilantro-1-Bunch/dp/B08731HWZV/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1',\n",
    "                    'https://www.amazon.com/Fresh-Brand-Organic-Basil-0-5/dp/B097F282FC/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2',\n",
    "                    'https://www.amazon.com/Fresh-Brand-Russet-Potatoes/dp/B07XW1TNXZ/ref=sr_1_1_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-1',\n",
    "                    'https://www.amazon.com/Taylor-Farms-65107-Spinach-Bag/dp/B00KMM8I6Y/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2',\n",
    "                    'https://www.amazon.com/Fresh-Brand-Vine-Tomatoes/dp/B086WX15TH/ref=sr_1_2_f3_wg?almBrandId=QW1hem9uIEZyZXNo&fpw=alm&s=amazonfresh&sr=1-2',\n",
    "                    # '',\n",
    "                    # '',\n",
    "                    # '',\n",
    "                    # '',\n",
    "                    # '',\n",
    "                    # '',\n",
    "                    # '',\n",
    "                    # '',\n",
    "                    # '',\n",
    "                    # '',\n",
    "                    # '',\n",
    "                     ]\n",
    "\n",
    "# sorted_sites = sorted(site_to_check)\n",
    "\n",
    "def write_html_file(site, proxy=None, folder_path='downloaded_html', prettify=False):\n",
    "    \n",
    "    try:\n",
    "        # Create the folder if it doesn't exist\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        # Extract filename between 2nd and 3rd backslashes (consider potential naming issues with special characters)\n",
    "        url_parts = urlparse(site).path.split('/')\n",
    "        if len(url_parts) >= 2:  # Ensure there are at least 2 parts (1 backslash)\n",
    "            filename = f\"{url_parts[1]}.html\"\n",
    "        else:\n",
    "            filename = f\"unknown_{site.split('/')[-1]}.html\"  # Fallback for malformed URLs\n",
    "\n",
    "        print(f'Downloading content for {site}...')\n",
    "\n",
    "        headers = {'User-Agent':\n",
    "                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',\n",
    "                   'Cache-Control': 'no-cache'}  # Disable caching in headers\n",
    "\n",
    "        response = requests.get(site, proxies={'http': proxy, 'https': proxy}, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Construct the full path to the file within the folder\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Downloaded content\n",
    "            html_content = response.text\n",
    "\n",
    "            if prettify:\n",
    "                # Prettify the HTML content using BeautifulSoup (optional)\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                pretty_html = soup.prettify()\n",
    "                html_content = pretty_html\n",
    "            else:\n",
    "                # Keep the content as-is\n",
    "                pass\n",
    "\n",
    "            with codecs.open(full_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(html_content)\n",
    "            print(f'{site} content written to {full_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {site} (Status code: {response.status_code})')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error downloading {site}: {e}')\n",
    "    \n",
    "\n",
    "# Site List Scrapping\n",
    "if __name__ == '__main__':\n",
    "    sites_to_check = site_to_check\n",
    "\n",
    "    # Call the function for each website (optionally with a proxy)\n",
    "    for site in site_to_check:\n",
    "        write_html_file(site)  # Without proxy\n",
    "        # write_html_file(site, proxy='http://your_proxy_address:port')  # With proxy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T23:01:14.464842Z",
     "start_time": "2024-05-22T23:00:31.424085Z"
    }
   },
   "id": "909236c925dc692c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Christopher-Ranch-White-Garlic-pack.html\n",
      "Data appended to existing CSV file.\n",
      "Christopher Ranch White Garlic 3 pack, 3 oz.\n",
      "Christopher-Ranch-White-Garlic-pack.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Church-Brothers-Farms-Broccoli-Florets.html\n",
      "Data appended to existing CSV file.\n",
      "Church Brothers Farms Broccoli Florets, 12 Oz\n",
      "Church-Brothers-Farms-Broccoli-Florets.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Dole-Organic-Bananas-Bag.html\n",
      "Data appended to existing CSV file.\n",
      "Organic Bananas\n",
      "Dole-Organic-Bananas-Bag.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Fresh-Brand-Organic-Basil-0-5.html\n",
      "Data appended to existing CSV file.\n",
      "Amazon Fresh Brand, Organic Basil, 0.5 Oz\n",
      "Fresh-Brand-Organic-Basil-0-5.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Fresh-Brand-Russet-Potatoes.html\n",
      "Data appended to existing CSV file.\n",
      "Amazon Fresh Brand, Russet Potatoes, 5 Lb\n",
      "Fresh-Brand-Russet-Potatoes.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Fresh-Brand-Stoplight-Bell-Peppers.html\n",
      "Data appended to existing CSV file.\n",
      "Amazon Fresh Brand, Stoplight Bell Peppers, 3 Count\n",
      "Fresh-Brand-Stoplight-Bell-Peppers.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Fresh-Brand-Vine-Tomatoes.html\n",
      "Data appended to existing CSV file.\n",
      "Amazon Fresh Brand, On The Vine Tomatoes, 24 Oz\n",
      "Fresh-Brand-Vine-Tomatoes.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Fresh-Brand-Whole-Carrots-16.html\n",
      "Data appended to existing CSV file.\n",
      "Amazon Fresh Brand, Whole Carrots, 16 Oz\n",
      "Fresh-Brand-Whole-Carrots-16.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Hass-Avocado-Large-Ready-Eat.html\n",
      "Data appended to existing CSV file.\n",
      "Medium Hass Avocado\n",
      "Hass-Avocado-Large-Ready-Eat.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Organic-Honeycrisp-Apple-One-Medium.html\n",
      "Data appended to existing CSV file.\n",
      "Organic Honeycrisp Apple, 1 Each\n",
      "Organic-Honeycrisp-Apple-One-Medium.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from produce-aisle-Lemon-One-Medium.html\n",
      "Data appended to existing CSV file.\n",
      "Lemon, 1 Each\n",
      "produce-aisle-Lemon-One-Medium.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from produce-aisle-mburring-Yellow-Onion.html\n",
      "Data appended to existing CSV file.\n",
      "Yellow Onion, 1 Each\n",
      "produce-aisle-mburring-Yellow-Onion.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from produce-aisle-Strawberries-1-lb.html\n",
      "Data appended to existing CSV file.\n",
      "Strawberries, 1 Lb\n",
      "produce-aisle-Strawberries-1-lb.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Tanimura-Antle-Cilantro-1-Bunch.html\n",
      "Data appended to existing CSV file.\n",
      "Tanimura & Antle, Cilantro, 1 Each\n",
      "Tanimura-Antle-Cilantro-1-Bunch.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Tanimura-Antle-Italian-Parsley-Bunch.html\n",
      "Data appended to existing CSV file.\n",
      "Italian Parsley, 1 Bunch\n",
      "Tanimura-Antle-Italian-Parsley-Bunch.html\n",
      "0 Organic-Honeycrisp-Apple-One-Medium\n",
      "Processing price data from Taylor-Farms-65107-Spinach-Bag.html\n",
      "Data appended to existing CSV file.\n",
      "Taylor Farms Spinach, 9 oz Bag\n",
      "Taylor-Farms-65107-Spinach-Bag.html\n"
     ]
    }
   ],
   "source": [
    "#The Final Product\n",
    "\n",
    "import subprocess\n",
    "from datetime import date  # Import date for today's date\n",
    "from bs4 import BeautifulSoup  # Import BeautifulSoup for parsing HTML\n",
    "import csv  # Import csv library for writing\n",
    "import os\n",
    "\n",
    "def find_index_in_library(title, site_to_check):\n",
    "  \"\"\"\n",
    "  This function checks if a title matches an entry in the site_to_check list and returns the corresponding index.\n",
    "\n",
    "  Args:\n",
    "      title: The title text to search for (usually from productTitle).\n",
    "      site_to_check: The list of website URLs.\n",
    "\n",
    "  Returns:\n",
    "      The index of the matching entry in site_to_check, or -1 if not found.\n",
    "  \"\"\"\n",
    "  for i, url in enumerate(site_to_check):\n",
    "    # Extract the product name from the URL (assuming it's part of the path)\n",
    "    url_parts = url.split('/')\n",
    "    if len(url_parts) >= 4:  # Ensure there are at least 4 parts (3 slashes)\n",
    "      product_name = url_parts[3].strip()  # Assuming product name is in the 4th position\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Compare the title (lowercase) with the extracted product name (lowercase)\n",
    "    if product_name and title.lower() == product_name.lower():\n",
    "      return i  # Return the index if there's a match\n",
    "    \n",
    "    print(i, product_name)\n",
    "    return -1\n",
    "  # return -1  # Return -1 if not found\n",
    "\n",
    "\n",
    "def check_price():\n",
    "    # Run the proxy rotation (assuming proxy_rotation.py is a separate script)\n",
    "    subprocess.run([\"python\", \"proxy_rotation.py\"])\n",
    "\n",
    "    # Path to the folder containing HTML files\n",
    "    folder_path = \"downloaded_html\"\n",
    "\n",
    "    \n",
    "    # Iterate over files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        \n",
    "        \n",
    "        if filename.endswith(\".html\"):  # Check if it's an HTML file\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            try:\n",
    "                # Open the file and process its contents\n",
    "                with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                    html_content = f.read()\n",
    "\n",
    "                    # Process the HTML content (extract price, etc.)\n",
    "                    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "                    # Access and process the parsed HTML using BeautifulSoup methods\n",
    "                    title = soup.find(id='productTitle').text.strip()\n",
    "                    price = soup.find(class_='a-price-whole').text.strip() + soup.find(class_='a-price-fraction').text.strip()\n",
    "                    today = date.today()\n",
    "                    \n",
    "                    index = find_index_in_library(title, site_to_check)\n",
    "                    link = site_to_check[index]\n",
    "                    \n",
    "                                       \n",
    "                \n",
    "                    print(f\"Processing price data from {filename}\")\n",
    "                    \n",
    "\n",
    "                    # ----------------------------------Writing into a csv ------------------------------------\n",
    "                    header = ['Title', 'Price', 'Date', 'Link']\n",
    "                    data = [[title, price, today, link]]\n",
    "\n",
    "                    # Check if the CSV file exists\n",
    "                    file_exists = os.path.isfile('amazon_web_scrapper_dataset.csv')\n",
    "\n",
    "                    if not file_exists:  # Create the CSV file if it doesn't exist\n",
    "                        with open('amazon_web_scrapper_dataset.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            writer.writerow(header)\n",
    "                            writer.writerows(data)\n",
    "                        print('CSV file created successfully!')\n",
    "                    else:  # Append data if the CSV file already exists\n",
    "                        with open('amazon_web_scrapper_dataset.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            writer.writerows(data)\n",
    "                        print('Data appended to existing CSV file.')\n",
    "                    \n",
    "            except Exception as e:  # Catch any exceptions during processing or writing\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                \n",
    "        print(title)\n",
    "        print(filename)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_price()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T01:09:57.096540Z",
     "start_time": "2024-05-23T01:09:55.710840Z"
    }
   },
   "id": "e821f9af12ad8547",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#-------------------------Automating the script into time intervals------------------------\n",
    "while(True):\n",
    "    write_html_file(site, proxy=None, folder_path='downloaded_html', prettify=False)\n",
    "    check_price()\n",
    "    time.sleep(10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed228a0bec23b66c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"amazon_web_scrapper_dataset.csv\" cleared successfully!\n",
      "CSV file \"amazon_web_scrapper_dataset.csv\" header preserved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def clear_csv(filename, keep_header=False):\n",
    "  \"\"\"\n",
    "  Clears the contents of a CSV file, optionally keeping the header line.\n",
    "\n",
    "  Args:\n",
    "      filename (str): The path to the CSV file.\n",
    "      keep_header (bool, optional): Whether to keep the header line (default: False).\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    if os.path.exists(filename):\n",
    "      with open(filename, 'r', newline='') as f:\n",
    "        lines = f.readlines()  # Read all lines into a list\n",
    "\n",
    "      if keep_header:  # Check only if keeping the header\n",
    "        lines = lines[2:]  # Skip the first line (header) if requested\n",
    "\n",
    "      else:\n",
    "        lines = []  # Empty list to clear the content\n",
    "\n",
    "      with open(filename, 'w', newline='') as f:\n",
    "        f.writelines(lines)  # Write the modified lines back to the file\n",
    "\n",
    "      if keep_header:\n",
    "        print(f'CSV file \"{filename}\" header preserved!')\n",
    "      else:\n",
    "        print(f'CSV file \"{filename}\" cleared successfully!')\n",
    "    else:\n",
    "      print(f'CSV file \"{filename}\" does not exist.')\n",
    "  except Exception as e:\n",
    "    print(f'Error clearing CSV file: {e}')\n",
    "\n",
    "# Example usage (clear everything)\n",
    "clear_csv('amazon_web_scrapper_dataset.csv')\n",
    "\n",
    "# Example usage (keep the header)\n",
    "clear_csv('amazon_web_scrapper_dataset.csv', keep_header=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T02:18:33.742430Z",
     "start_time": "2024-05-21T02:18:33.735163Z"
    }
   },
   "id": "57b466008b2d131d",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (155190715.py, line 63)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[23], line 63\u001B[1;36m\u001B[0m\n\u001B[1;33m    soup = BeautifulSoup(html content, 'html.parser')\u001B[0m\n\u001B[1;37m                         ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import date  # Import date for today's date\n",
    "from bs4 import BeautifulSoup  # Import BeautifulSoup for parsing HTML\n",
    "import csv  # Import csv library for writing\n",
    "import os\n",
    "\n",
    "def find_index_in_library(title, site_to_check, partial_match=False):\n",
    "  \"\"\"\n",
    "  This function checks if a title matches an entry in the site_to_check list and returns the corresponding index.\n",
    "  Optionally, it can perform a partial title match.\n",
    "\n",
    "  Args:\n",
    "      title: The title text to search for (usually from productTitle).\n",
    "      site_to_check: The list of website URLs.\n",
    "      partial_match: Boolean flag indicating whether to perform a partial title match (default: False).\n",
    "\n",
    "  Returns:\n",
    "      The index of the matching entry in site_to_check, or -1 if not found.\n",
    "  \"\"\"\n",
    "  for i, url in enumerate(site_to_check):\n",
    "    # Extract the product name from the URL (assuming it's part of the path)\n",
    "    url_parts = url.split('/')\n",
    "    if len(url_parts) >= 4:  # Ensure there are at least 4 parts (3 slashes)\n",
    "      product_name = url_parts[3].strip()  # Assuming product name is in the 4th position\n",
    "    else:\n",
    "      product_name = None\n",
    "\n",
    "    # Compare titles (lowercase) with partial match option\n",
    "    if product_name and (title.lower() == product_name.lower() if not partial_match else title.lower() in product_name.lower()):\n",
    "      return i  # Return the index if there's a match\n",
    "  return -1  # Return -1 if not found\n",
    "\n",
    "\n",
    "def check_price(site_to_check):\n",
    "    # Proxy Rotation (Optional):\n",
    "    # Uncomment the following line if you have a separate script (`proxy_rotation.py`) for handling proxy rotation\n",
    "    # subprocess.run([\"python\", \"proxy_rotation.py\"])\n",
    "\n",
    "    # Path to the folder containing HTML files\n",
    "    folder_path = \"downloaded_html\"\n",
    "\n",
    "    # Open CSV file for writing in append mode (create if it doesn't exist)\n",
    "    with open('amazon_web_scrapper_dataset.csv', 'a', newline='', encoding='utf-8') as csvfile:  # Open in append mode\n",
    "        writer = csv.writer(csvfile)\n",
    "        header = ['Title', 'Price', 'Date', 'Link']\n",
    "\n",
    "        # Check if the CSV needs a header (only write header if file is empty)\n",
    "        if csvfile.tell() == 0:\n",
    "            writer.writerow(header)\n",
    "\n",
    "        counter = 0  # Link Counter\n",
    "        # Iterate over files in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".html\"):  # Check if it's an HTML file\n",
    "                full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                try:\n",
    "                    # Open the file and process its contents\n",
    "                    with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                        html_content = f.read()\n",
    "\n",
    "                        # Process the HTML content (extract price, etc.)\n",
    "                        soup = BeautifulSoup(html content, 'html.parser')\n",
    "\n",
    "                        # Access and process the parsed HTML using BeautifulSoup methods\n",
    "                        title = soup.find(id='productTitle').text.strip()\n",
    "                        price_element = soup.find(class_='a-price-whole')  # Replace with your price element selector\n",
    "                        if price_element:\n",
    "                            whole_price = price_element.text.strip()\n",
    "                        else:\n",
    "                            whole_price = None\n",
    "\n",
    "                        price_element = soup.find(class_='a-price-fraction')  # Replace with your price element selector\n",
    "                        if price_element:\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    check_price(site_to_check)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T00:02:31.412925Z",
     "start_time": "2024-05-23T00:02:31.404046Z"
    }
   },
   "id": "17e1fb9a10763d76",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8d6837302b3b11b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
